import tensorflow as tf
from tensorflow.keras import backend as K
import tensorflow.keras.layers as L
from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping

NUM_FID = 18
NUM_TARGET_FIDS = 14
NUM_TARGET_CONCS=13
TRAIN_DATA_SIZE =1024
LEARNING_RATE=5e-4

# Default distribution strategy for single GPU is:
#    strategy = tf.distribute.get_strategy()
# if TPU, use the following code to creat a distributed strategy on a tpu:
#    tf.config.experimental_connect_to_cluster(tpu)
#    tf.tpu.experimental.initialize_tpu_system(tpu)
#    strategy = tf.distribute.experimental.TPUStrategy(tpu)
#
# The input for training is a tuple ({'FID input':total_signal}, target). The target here is a dictionary {'concentrations':concentrations, 'target_individual_signals':
# individual_signals, 'target_tatal_signal': total_signal, 'phase':phase, 'frequency':frequency}. Different from that in FID input, the target total signal is free of
# noise and extraneous peaks in order to compute total FID loss (which can be disabled by setting the loss_weight = 0). All item values in the dictionaries are 
# tf.float32 tensors and have the unbatched formats:(32, 2048, 2), (NUM_TARGET_CONCS), (32, 2048, NUM_TARGET_FIDS*2), (32, 2048, 2), (2), (2), respectively. The phase
# means (cos(angle), sin(angle)). The two values in the frequency are the frequency offsets of water and metabolites, respectively. Performing inference only needs the
# input {'FID input': total_signal}. The key names in the target dictionary need to match the output names specified in the model. Use tf Dataset to prepare data and
# tf Dataloader to batch and load data. The final FID input has the format (BATCH_SIZE, 32, 2048, 2).

REPLICA_BATCH_SIZE=BATCH_SIZE//strategy.num_replicas_in_sync

def wavenet(x, filters, dilations=8, kernel_size=5):
    dilation_rates = [2**i for i in range(dilations)]
    x = L.Conv1D(filters = filters,
                            kernel_size = 1,
                            padding = 'same')(x)
    res_x = x
    for dilation_rate in dilation_rates:
        tanh_out = L.Conv1D(filters = filters,
                      kernel_size = kernel_size,
                      padding = 'same',
                      activation = 'tanh',
                      dilation_rate = dilation_rate)(x)
        sigm_out = L.Conv1D(filters = filters,
                      kernel_size = kernel_size,
                      padding = 'same',
                      activation = 'sigmoid',
                      dilation_rate = dilation_rate)(x)
        x = L.Multiply()([tanh_out, sigm_out])

        x = L.Conv1D(filters = filters,   kernel_size = 1,padding = 'same')(x)

        res_x = L.Add()([res_x, x])

    return res_x


def deepJPRESS(dims, echoes=32, points=2048, dilation_depth=8,
               num_concentrations=NUM_TARGET_CONCS, num_fids=NUM_TARGET_FIDS):
          input = L.Input(shape=(echoes, points, 2), name='input FID')
          x = tf.reshape(input, (-1,points,2))
          x = x - tf.math.reduce_mean(x, axis=1, keepdims=True)

          #encoder block1
          x = wavenet(x,dims, dilation_depth)
          x = L.LayerNormalization(axis=-1)(x)
          wavenet_out = L.Activation('relu')(x)
          to_decoder1 = tf.stop_gradient(wavenet_out)

          #x = L.GlobalAveragePooling1D()(wavenet_out)
          x = wavenet_out[:,0:64,:]
          x = tf.reduce_mean(x, 1)

          x = tf.reshape(x,(-1,echoes, dims))
          x = L.Bidirectional(L.GRU(dims,return_sequences=True))(x)
          x = L.Conv1D(dims, kernel_size=1, padding='same', use_bias=False)(x)
          x = L.LayerNormalization(axis=-1)(x)
          x = L.Activation('relu')(x)
          x = tf.reshape(x, (-1, dims))
          x = L.RepeatVector(points)(x)
          x = tf.concat([wavenet_out, x],-1)


          #encoder block2
          x = wavenet(x, dims, dilation_depth)
          x = L.LayerNormalization(axis=-1)(x)
          wavenet_out = L.Activation('relu')(x)
          to_decoder2 = tf.stop_gradient(wavenet_out)

          #x = L.GlobalAveragePooling1D()(wavenet_out)
          x = wavenet_out[:,0:64,:]
          x = tf.reduce_mean(x, 1)

          x = tf.reshape(x,(-1,echoes, dims))
          x = L.Bidirectional(L.GRU(dims, return_sequences=True))(x)
          x = L.Conv1D(dims, kernel_size=1, padding='same', use_bias=False)(x)
          x = L.LayerNormalization(axis=-1)(x)
          x = L.Activation('relu')(x)
          x = tf.reshape(x, (-1, dims))
          x = L.RepeatVector(points)(x)
          x = tf.concat([wavenet_out, x],-1)

          #encoder block3
          x = wavenet(x, dims, dilation_depth)
          x = L.LayerNormalization(axis=-1)(x)
          wavenet_out = L.Activation('relu')(x)
          to_decoder3 = tf.stop_gradient(wavenet_out)

          #x = L.GlobalAveragePooling1D()(wavenet_out)
          x = wavenet_out[:,0:64,:]
          x = tf.reduce_mean(x, 1)

          x = tf.reshape(x,(-1,echoes, dims))
          x = L.Bidirectional(L.GRU(dims, return_sequences=True))(x)
          x = L.Conv1D(dims, kernel_size=1, padding='same', use_bias=False)(x)
          x = L.LayerNormalization(axis=-1)(x)
          x = L.Activation('relu')(x)
          x = tf.reshape(x, (-1, dims))
          x = L.RepeatVector(points)(x)
          x = tf.concat([wavenet_out, x],-1)

          #encoder block4
          x = wavenet(x, dims, dilation_depth)
          x = L.LayerNormalization(axis=-1)(x)
          wavenet_out = L.Activation('relu')(x)


          x = wavenet_out[:,0:64,:]
          x = tf.reduce_mean(x, 1)
          x = tf.reshape(x,(-1,echoes, dims))
          x = L.Bidirectional(L.GRU(dims,return_sequences=True))(x)
          x = L.Bidirectional(L.GRU(dims,return_sequences=True))(x)
          x = L.Conv1D(dims, kernel_size=1, padding='same', use_bias=False)(x)
          x = L.LayerNormalization(axis=-1)(x)
          x = L.Activation('relu')(x)

          fid_amplitude = L.Dense(num_concentrations, name='target_fid_amplitude')(x)
          features = L.GlobalAveragePooling1D()(x)
          concentrations = L.Dense(num_concentrations, name='concentration')(features)
          t2 = L.Dense(num_concentrations, name='t2')(features)


          #decoder
          x = tf.reshape(x, (-1, dims))
          x = L.RepeatVector(points)(x)
          x = tf.concat([x, wavenet_out, to_decoder3, to_decoder2, to_decoder1], -1)
          x = wavenet(x, dims, dilation_depth)
          x = L.LayerNormalization(axis=-1)(x)
          x = L.Activation('relu')(x)
          x = wavenet(x, dims, dilation_depth)

          #outputs of individual FIDs, total FIDs
          x = tf.reshape(x,(-1,echoes, points, dims))
          target_individual_signal = L.Dense(num_fids*2, name='target_individual_signal')(x)

          model = tf.keras.Model(inputs=input, outputs= [target_individual_signal, t2, concentrations, fid_amplitude])

          return model


replica_batch_size=BATCH_SIZE//strategy.num_replicas_in_sync

class OneCycleSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, learning_rate, min_lr, batch_size, epochs, num_samples, warmup=0.2, hold=0.1):
        self.max_lr = learning_rate
        self.min_lr = min_lr
        self.batch_size = batch_size
        self.total_steps = 1.0*epochs*(num_samples//batch_size+1)
        self.warmup_steps = warmup*epochs*(num_samples//batch_size+1)
        self.hold_steps = hold*epochs*(num_samples//batch_size+1)
        self.decay_steps = self.total_steps - self.warmup_steps-self.hold_steps
    @tf.function
    def __call__(self, step):
        step = tf.cast(step, tf.float32)
        if step < self.warmup_steps:
            lr = self.max_lr*step/(self.warmup_steps+1.0)
        elif step < self.warmup_steps + self.hold_steps:
            lr = self.max_lr
        else:
            progress = (step-self.warmup_steps-self.hold_steps)/self.decay_steps
            lr = self.max_lr*0.5*(1.0 + tf.math.cos(math.pi*progress))

        return tf.math.maximum(lr, self.min_lr)

lr_scheduler = OneCycleSchedule(LEARNING_RATE, 1e-8, BATCH_SIZE, EPOCHS, TRAIN_DATA_SIZE)


def make_model(dims=128):

    K.clear_session()
    with strategy.scope():

        #[Water_fid, NAA_fid, Cr_fid, Cr2_fid, Cho_fid, mI_fid, Glu_fid, Gln_fid, GSH_fid, GABA_fid, Asp_fid, Tau_fid, Lac_fid, NAAG_fid, HG2_fid]
        def individual_signal_loss(y_true, y_pred):
          weight = tf.constant(2*[0.05]  +  2*(NUM_TARGET_FIDS-1)*[1.0])
          weight = tf.reshape(weight, (1,1,1,NUM_TARGET_FIDS*2))
          y_true  = y_true*weight
          y_pred = y_pred*weight
          y_true = tf.reshape(y_true, (y_true.shape[0], -1))
          y_pred = tf.reshape(y_pred, (y_true.shape[0], -1))
          sig_loss_replica = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)
          return tf.reduce_sum(sig_loss_replica(y_true, y_pred))/replica_batch_size

        def t2_loss(y_true, y_pred):
          t2_loss_replica = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)
          return tf.reduce_sum(t2_loss_replica(y_true, y_pred))/replica_batch_size

        def conc_loss(y_true, y_pred):
          weight = tf.constant([0.05] + (NUM_TARGET_CONCS-1)*[1.0])
          weight = tf.reshape(weight, (1,-1))

          y_true = weight*y_true
          y_pred = weight*y_pred
          y_pred = tf.clip_by_value(y_pred, 0, 1e5)

          met_loss_replica = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)
          return tf.reduce_sum(met_loss_replica(y_true, y_pred))/replica_batch_size

        def fid_amplitude_loss(y_true, y_pred):
          weight = tf.constant([0.05] + (NUM_TARGET_CONCS-1)*[1.0])
          weight = tf.reshape(weight, (1,1,-1))

          y_true = weight*y_true
          y_pred = weight*y_pred
          y_pred = tf.clip_by_value(y_pred, 0, 1e5)

          loss_replica = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)
          return tf.reduce_sum(loss_replica(y_true, y_pred))/replica_batch_size/32


        opt = tf.keras.optimizers.Adam(learning_rate = lr_scheduler)
        model =deepJPRESS(dims)
        model.compile(optimizer=opt, loss=[individual_signal_loss, t2_loss, conc_loss, fid_amplitude_loss],
                        loss_weights=[20.0, 1.0, 0.3, 0.1])


        model.summary()

        return model
