import math, re, os

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
from scipy.fftpack import fft, fftshift, ifft
from glob import glob
import random
from tqdm.notebook import tqdm

import tensorflow as tf
from tensorflow.keras import backend as K
import tensorflow.keras.layers as L
from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping
from sklearn import metrics

import warnings
warnings.filterwarnings("ignore")
NUM_FID = 18
NUM_TARGET_FIDS = 14
NUM_TARGET_CONCS=13
TRAIN_DATA_SIZE =1024
LEARNING_RATE=5e-4


def wavenet(x, filters, dilations=8, kernel_size=5):
    dilation_rates = [2**i for i in range(dilations)]
    x = L.Conv1D(filters = filters,
                            kernel_size = 1,
                            padding = 'same')(x)
    res_x = x
    for dilation_rate in dilation_rates:
        tanh_out = L.Conv1D(filters = filters,
                      kernel_size = kernel_size,
                      padding = 'same',
                      activation = 'tanh',
                      dilation_rate = dilation_rate)(x)
        sigm_out = L.Conv1D(filters = filters,
                      kernel_size = kernel_size,
                      padding = 'same',
                      activation = 'sigmoid',
                      dilation_rate = dilation_rate)(x)
        x = L.Multiply()([tanh_out, sigm_out])

        x = L.Conv1D(filters = filters,   kernel_size = 1,padding = 'same')(x)

        res_x = L.Add()([res_x, x])

    return res_x


def deepJPRESS(dims, echoes=32, points=2048, dilation_depth=8,
               num_concentrations=NUM_TARGET_CONCS, num_fids=NUM_TARGET_FIDS):
          input = L.Input(shape=(echoes, points, 2), name='input FID')
          x = tf.reshape(input, (-1,points,2))
          x = x - tf.math.reduce_mean(x, axis=1, keepdims=True)

          #encoder block1
          x = wavenet(x,dims, dilation_depth)
          x = L.LayerNormalization(axis=-1)(x)
          wavenet_out = L.Activation('relu')(x)
          to_decoder1 = tf.stop_gradient(wavenet_out)

          #x = L.GlobalAveragePooling1D()(wavenet_out)
          x = wavenet_out[:,0:64,:]
          x = tf.reduce_mean(x, 1)

          x = tf.reshape(x,(-1,echoes, dims))
          x = L.Bidirectional(L.GRU(dims,return_sequences=True))(x)
          x = L.Conv1D(dims, kernel_size=1, padding='same', use_bias=False)(x)
          x = L.LayerNormalization(axis=-1)(x)
          x = L.Activation('relu')(x)
          x = tf.reshape(x, (-1, dims))
          x = L.RepeatVector(points)(x)
          x = tf.concat([wavenet_out, x],-1)


          #encoder block2
          x = wavenet(x, dims, dilation_depth)
          x = L.LayerNormalization(axis=-1)(x)
          wavenet_out = L.Activation('relu')(x)
          to_decoder2 = tf.stop_gradient(wavenet_out)

          #x = L.GlobalAveragePooling1D()(wavenet_out)
          x = wavenet_out[:,0:64,:]
          x = tf.reduce_mean(x, 1)

          x = tf.reshape(x,(-1,echoes, dims))
          x = L.Bidirectional(L.GRU(dims, return_sequences=True))(x)
          x = L.Conv1D(dims, kernel_size=1, padding='same', use_bias=False)(x)
          x = L.LayerNormalization(axis=-1)(x)
          x = L.Activation('relu')(x)
          x = tf.reshape(x, (-1, dims))
          x = L.RepeatVector(points)(x)
          x = tf.concat([wavenet_out, x],-1)

          #encoder block3
          x = wavenet(x, dims, dilation_depth)
          x = L.LayerNormalization(axis=-1)(x)
          wavenet_out = L.Activation('relu')(x)
          to_decoder3 = tf.stop_gradient(wavenet_out)

          #x = L.GlobalAveragePooling1D()(wavenet_out)
          x = wavenet_out[:,0:64,:]
          x = tf.reduce_mean(x, 1)

          x = tf.reshape(x,(-1,echoes, dims))
          x = L.Bidirectional(L.GRU(dims, return_sequences=True))(x)
          x = L.Conv1D(dims, kernel_size=1, padding='same', use_bias=False)(x)
          x = L.LayerNormalization(axis=-1)(x)
          x = L.Activation('relu')(x)
          x = tf.reshape(x, (-1, dims))
          x = L.RepeatVector(points)(x)
          x = tf.concat([wavenet_out, x],-1)

          #encoder block4
          x = wavenet(x, dims, dilation_depth)
          x = L.LayerNormalization(axis=-1)(x)
          wavenet_out = L.Activation('relu')(x)


          x = wavenet_out[:,0:64,:]
          x = tf.reduce_mean(x, 1)
          x = tf.reshape(x,(-1,echoes, dims))
          x = L.Bidirectional(L.GRU(dims,return_sequences=True))(x)
          x = L.Bidirectional(L.GRU(dims,return_sequences=True))(x)
          x = L.Conv1D(dims, kernel_size=1, padding='same', use_bias=False)(x)
          x = L.LayerNormalization(axis=-1)(x)
          x = L.Activation('relu')(x)

          fid_amplitude = L.Dense(num_concentrations, name='target_fid_amplitude')(x)
          features = L.GlobalAveragePooling1D()(x)
          concentrations = L.Dense(num_concentrations, name='concentration')(features)
          t2 = L.Dense(num_concentrations, name='t2')(features)


          #decoder
          x = tf.reshape(x, (-1, dims))
          x = L.RepeatVector(points)(x)
          x = tf.concat([x, wavenet_out, to_decoder3, to_decoder2, to_decoder1], -1)
          x = wavenet(x, dims, dilation_depth)
          x = L.LayerNormalization(axis=-1)(x)
          x = L.Activation('relu')(x)
          x = wavenet(x, dims, dilation_depth)

          #outputs of individual FIDs, total FIDs
          x = tf.reshape(x,(-1,echoes, points, dims))
          target_individual_signal = L.Dense(num_fids*2, name='target_individual_signal')(x)

          model = tf.keras.Model(inputs=input, outputs= [target_individual_signal, t2, concentrations, fid_amplitude])

          return model


replica_batch_size=BATCH_SIZE//strategy.num_replicas_in_sync

class OneCycleSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, learning_rate, min_lr, batch_size, epochs, num_samples, warmup=0.2, hold=0.1):
        self.max_lr = learning_rate
        self.min_lr = min_lr
        self.batch_size = batch_size
        self.total_steps = 1.0*epochs*(num_samples//batch_size+1)
        self.warmup_steps = warmup*epochs*(num_samples//batch_size+1)
        self.hold_steps = hold*epochs*(num_samples//batch_size+1)
        self.decay_steps = self.total_steps - self.warmup_steps-self.hold_steps
    @tf.function
    def __call__(self, step):
        step = tf.cast(step, tf.float32)
        if step < self.warmup_steps:
            lr = self.max_lr*step/(self.warmup_steps+1.0)
        elif step < self.warmup_steps + self.hold_steps:
            lr = self.max_lr
        else:
            progress = (step-self.warmup_steps-self.hold_steps)/self.decay_steps
            lr = self.max_lr*0.5*(1.0 + tf.math.cos(math.pi*progress))

        return tf.math.maximum(lr, self.min_lr)

lr_scheduler = OneCycleSchedule(LEARNING_RATE, 1e-8, BATCH_SIZE, EPOCHS, TRAIN_DATA_SIZE)


def make_model(dims=128):

    K.clear_session()
    with strategy.scope():

        #[Water_fid, NAA_fid, Cr_fid, Cr2_fid, Cho_fid, mI_fid, Glu_fid, Gln_fid, GSH_fid, GABA_fid, Asp_fid, Tau_fid, Lac_fid, NAAG_fid, HG2_fid]
        def individual_signal_loss(y_true, y_pred):
          weight = tf.constant(2*[0.05]  +  2*(NUM_TARGET_FIDS-1)*[1.0])
          weight = tf.reshape(weight, (1,1,1,NUM_TARGET_FIDS*2))
          y_true  = y_true*weight
          y_pred = y_pred*weight
          y_true = tf.reshape(y_true, (y_true.shape[0], -1))
          y_pred = tf.reshape(y_pred, (y_true.shape[0], -1))
          sig_loss_replica = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)
          return tf.reduce_sum(sig_loss_replica(y_true, y_pred))/replica_batch_size

        def t2_loss(y_true, y_pred):
          t2_loss_replica = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)
          return tf.reduce_sum(t2_loss_replica(y_true, y_pred))/replica_batch_size

        def conc_loss(y_true, y_pred):
          weight = tf.constant([0.05] + (NUM_TARGET_CONCS-1)*[1.0])
          weight = tf.reshape(weight, (1,-1))

          y_true = weight*y_true
          y_pred = weight*y_pred
          y_pred = tf.clip_by_value(y_pred, 0, 1e5)

          met_loss_replica = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)
          return tf.reduce_sum(met_loss_replica(y_true, y_pred))/replica_batch_size

        def fid_amplitude_loss(y_true, y_pred):
          weight = tf.constant([0.05] + (NUM_TARGET_CONCS-1)*[1.0])
          weight = tf.reshape(weight, (1,1,-1))

          y_true = weight*y_true
          y_pred = weight*y_pred
          y_pred = tf.clip_by_value(y_pred, 0, 1e5)

          loss_replica = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)
          return tf.reduce_sum(loss_replica(y_true, y_pred))/replica_batch_size/32


        opt = tf.keras.optimizers.Adam(learning_rate = lr_scheduler)
        model =deepJPRESS(dims)
        model.compile(optimizer=opt, loss=[individual_signal_loss, t2_loss, conc_loss, fid_amplitude_loss],
                        loss_weights=[20.0, 1.0, 0.3, 0.1])


        #model.summary()

        return model
